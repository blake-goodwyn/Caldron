2024-05-30 16:36:30,906 - cauldron - INFO   - cauldron_app.py - Initializing Cauldron Application
2024-05-30 16:36:32,203 - cauldron - INFO   - agent_defs.py - Creating all agents.
2024-05-30 16:36:32,203 - cauldron - INFO   - agent_defs.py - Creating supervisor agent: ConductorAgent
2024-05-30 16:36:32,205 - cauldron - INFO   - agent_defs.py - Agent ConductorAgent created with node first=ChatPromptTemplate(input_variables=['messages'], input_types={'messages': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'options': "['FINISH', 'RecipeResearchAgent', 'FeedbackAgent', 'ModificationsAgent', 'DevelopmentTrackerAgent', 'PeripheralFeedbackAgent']", 'team_members': 'RecipeResearchAgent, FeedbackAgent, ModificationsAgent, DevelopmentTrackerAgent, PeripheralFeedbackAgent'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a supervisor tasked with managing user requests related to recipe development between the following workers: FlavorProfileAgent, NutrionalAnalysisAgent, CostAvailabilityAgent, FeedbackAgent, SQLAgent, ModificationsAgent, DevelopmentTrackerAgent, PeripheralFeedbackAgent. When a user request is received, respond with a confirmation and specify which worker(s) will act next. Each worker will perform a task and respond with their results and status. When all tasks are complete, respond with FINISH. Ensure all communication follows Pydantic standards and maintain a clear and concise log of all communications for transparency.')), MessagesPlaceholder(variable_name='messages'), SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['options'], template='Given the conversation above, who should act next? Or should we FINISH? Select one of: {options}'))]) middle=[RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000002B0CBC54B00>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000002B0CBC562D0>, openai_api_key=SecretStr('**********'), openai_proxy=''), kwargs={'functions': [{'name': 'route', 'description': 'Select the next role.', 'parameters': {'title': 'routeSchema', 'type': 'object', 'properties': {'next': {'title': 'Next', 'anyOf': [{'enum': ['FINISH', 'RecipeResearchAgent', 'FeedbackAgent', 'ModificationsAgent', 'DevelopmentTrackerAgent', 'PeripheralFeedbackAgent']}]}}, 'required': ['next']}}], 'function_call': {'name': 'route'}})] last=JsonOutputFunctionsParser()
2024-05-30 16:36:32,209 - cauldron - INFO   - agent_defs.py - Creating supervisor agent: RecipeResearchAgent
2024-05-30 16:36:32,209 - cauldron - INFO   - agent_defs.py - Agent RecipeResearchAgent created with node first=ChatPromptTemplate(input_variables=['messages'], input_types={'messages': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'options': "['FINISH', 'FlavorProfileAgent', 'NutrionalAnalysisAgent', 'CostAvailabilityAgent', 'SQLAgent']", 'team_members': 'FlavorProfileAgent, NutrionalAnalysisAgent, CostAvailabilityAgent, SQLAgent'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are the SupervisorAgent overseeing the following nodes in the Cauldron application: FlavorProfileAgent, NutrionalAnalysisAgent, CostAvailabilityAgent, and SQLAgent. Your task is to coordinate their efforts to ensure seamless recipe development. When a user request is received, assign tasks to the appropriate agents based on their specializations. Ensure each agent adheres to Pydantic standards in their analysis and formatting of outputs. Collect and review the results from each agent, resolving any detected looping issues or requests for additional input. Once all agents have completed their tasks, compile a comprehensive report summarizing their findings and the overall status of the recipe development process. Maintain a clear and concise log of all communications for transparency.')), MessagesPlaceholder(variable_name='messages'), SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['options'], template='Given the conversation above, who should act next? Or should we FINISH? Select one of: {options}'))]) middle=[RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000002B0CBC54B00>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000002B0CBC562D0>, openai_api_key=SecretStr('**********'), openai_proxy=''), kwargs={'functions': [{'name': 'route', 'description': 'Select the next role.', 'parameters': {'title': 'routeSchema', 'type': 'object', 'properties': {'next': {'title': 'Next', 'anyOf': [{'enum': ['FINISH', 'FlavorProfileAgent', 'NutrionalAnalysisAgent', 'CostAvailabilityAgent', 'SQLAgent']}]}}, 'required': ['next']}}], 'function_call': {'name': 'route'}})] last=JsonOutputFunctionsParser()
2024-05-30 16:36:32,212 - cauldron - INFO   - agent_defs.py - Creating agent: FlavorProfileAgent
2024-05-30 16:36:32,715 - cauldron - INFO   - agent_defs.py - Agent FlavorProfileAgent created with node functools.partial(<function agent_node at 0x000002B0C9801D00>, agent=ChatPromptTemplate(input_variables=['messages'], input_types={'messages': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'system_message': 'You are the Flavor Profiling node for the Cauldron application. Your task is to analyze the flavor profiles of the ingredients provided and suggest combinations that enhance the overall taste of the recipe. Ensure your analysis adheres to Pydantic standards and format your output accordingly. Once the analysis is complete, forward your results to the relevant nodes (e.g., Nutritional Balancing, Recipe Modification Manager). If you detect a looping issue or need further input, communicate this clearly and concisely.', 'tool_names': 'get_foundational_recipe, suggest_mod'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['system_message', 'tool_names'], template="You are a helpful AI assistant, collaborating with other assistants. Use the provided tools to progress towards answering the question. If you are unable to fully answer, that's OK, another assistant with different tools  will help where you left off. Execute what you can to make progress. If you or any of the other assistants have the final answer or deliverable, prefix your response with FINAL ANSWER so the team knows to stop. You have access to the following tools: {tool_names}.\n\n Your responsibility is the following:\n{system_message}")), MessagesPlaceholder(variable_name='messages')])
| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000002B0CBC54B00>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000002B0CBC562D0>, openai_api_key=SecretStr('**********'), openai_proxy=''), kwargs={'tools': [{'type': 'function', 'function': {'name': 'get_foundational_recipe', 'description': "get_foundational_recipe(graph_file: Annotated[str, 'The filename for the recipe graph.']) -> Annotated[str, 'The JSON representation of the current foundational recipe.'] - Get the JSON representation of the current foundational recipe.", 'parameters': {'type': 'object', 'properties': {'graph_file': {'type': 'string'}}, 'required': ['graph_file']}}}, {'type': 'function', 'function': {'name': 'suggest_mod', 'description': "suggest_mod(mod_json: Annotated[str, 'The modification JSON to be suggested.'], mods_list_file: Annotated[str, 'The filename for the mods list.'] = 'mods_list.pkl') -> Annotated[str, 'The updated list of modifications.'] - Suggest a new modification to be added to the mods list.", 'parameters': {'type': 'object', 'properties': {'mod_json': {'type': 'string'}, 'mods_list_file': {'default': 'mods_list.pkl', 'type': 'string'}}, 'required': ['mod_json']}}}]}), name='FlavorProfileAgent')
2024-05-30 16:36:32,719 - cauldron - INFO   - agent_defs.py - Creating agent: NutrionalAnalysisAgent
2024-05-30 16:36:32,721 - cauldron - INFO   - agent_defs.py - Agent NutrionalAnalysisAgent created with node functools.partial(<function agent_node at 0x000002B0C9801D00>, agent=ChatPromptTemplate(input_variables=['messages'], input_types={'messages': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'system_message': 'You are the Nutritional Balancing node for the Cauldron application. Your task is to evaluate the nutritional content of the ingredients provided and ensure the recipe meets specific nutritional guidelines. Make suggestions for ingredient adjustments to achieve a balanced nutrient profile. Format all outputs according to Pydantic standards and forward your results to the relevant nodes (e.g., Flavor Profiling, Recipe Modification Manager). Address any looping issues or additional input needs clearly and concisely.', 'tool_names': 'get_foundational_recipe, suggest_mod'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['system_message', 'tool_names'], template="You are a helpful AI assistant, collaborating with other assistants. Use the provided tools to progress towards answering the question. If you are unable to fully answer, that's OK, another assistant with different tools  will help where you left off. Execute what you can to make progress. If you or any of the other assistants have the final answer or deliverable, prefix your response with FINAL ANSWER so the team knows to stop. You have access to the following tools: {tool_names}.\n\n Your responsibility is the following:\n{system_message}")), MessagesPlaceholder(variable_name='messages')])
| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000002B0CBC54B00>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000002B0CBC562D0>, openai_api_key=SecretStr('**********'), openai_proxy=''), kwargs={'tools': [{'type': 'function', 'function': {'name': 'get_foundational_recipe', 'description': "get_foundational_recipe(graph_file: Annotated[str, 'The filename for the recipe graph.']) -> Annotated[str, 'The JSON representation of the current foundational recipe.'] - Get the JSON representation of the current foundational recipe.", 'parameters': {'type': 'object', 'properties': {'graph_file': {'type': 'string'}}, 'required': ['graph_file']}}}, {'type': 'function', 'function': {'name': 'suggest_mod', 'description': "suggest_mod(mod_json: Annotated[str, 'The modification JSON to be suggested.'], mods_list_file: Annotated[str, 'The filename for the mods list.'] = 'mods_list.pkl') -> Annotated[str, 'The updated list of modifications.'] - Suggest a new modification to be added to the mods list.", 'parameters': {'type': 'object', 'properties': {'mod_json': {'type': 'string'}, 'mods_list_file': {'default': 'mods_list.pkl', 'type': 'string'}}, 'required': ['mod_json']}}}]}), name='NutrionalAnalysisAgent')
2024-05-30 16:36:32,725 - cauldron - INFO   - agent_defs.py - Creating agent: CostAvailabilityAgent
2024-05-30 16:36:32,727 - cauldron - INFO   - agent_defs.py - Agent CostAvailabilityAgent created with node functools.partial(<function agent_node at 0x000002B0C9801D00>, agent=ChatPromptTemplate(input_variables=['messages'], input_types={'messages': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'system_message': 'You are the Cost & Availability node for the Cauldron application. Your task is to assess the cost and availability of the ingredients provided. Analyze market trends, regional availability, and pricing data to suggest the most cost-effective and available options. Ensure all communication follows Pydantic standards and format your output accordingly. Forward your results to the relevant nodes (e.g., Nutritional Balancing, Recipe Modification Manager). Clearly communicate if additional input or a change in direction is needed.', 'tool_names': 'get_foundational_recipe, suggest_mod'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['system_message', 'tool_names'], template="You are a helpful AI assistant, collaborating with other assistants. Use the provided tools to progress towards answering the question. If you are unable to fully answer, that's OK, another assistant with different tools  will help where you left off. Execute what you can to make progress. If you or any of the other assistants have the final answer or deliverable, prefix your response with FINAL ANSWER so the team knows to stop. You have access to the following tools: {tool_names}.\n\n Your responsibility is the following:\n{system_message}")), MessagesPlaceholder(variable_name='messages')])
| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000002B0CBC54B00>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000002B0CBC562D0>, openai_api_key=SecretStr('**********'), openai_proxy=''), kwargs={'tools': [{'type': 'function', 'function': {'name': 'get_foundational_recipe', 'description': "get_foundational_recipe(graph_file: Annotated[str, 'The filename for the recipe graph.']) -> Annotated[str, 'The JSON representation of the current foundational recipe.'] - Get the JSON representation of the current foundational recipe.", 'parameters': {'type': 'object', 'properties': {'graph_file': {'type': 'string'}}, 'required': ['graph_file']}}}, {'type': 'function', 'function': {'name': 'suggest_mod', 'description': "suggest_mod(mod_json: Annotated[str, 'The modification JSON to be suggested.'], mods_list_file: Annotated[str, 'The filename for the mods list.'] = 'mods_list.pkl') -> Annotated[str, 'The updated list of modifications.'] - Suggest a new modification to be added to the mods list.", 'parameters': {'type': 'object', 'properties': {'mod_json': {'type': 'string'}, 'mods_list_file': {'default': 'mods_list.pkl', 'type': 'string'}}, 'required': ['mod_json']}}}]}), name='CostAvailabilityAgent')
2024-05-30 16:36:32,731 - cauldron - INFO   - agent_defs.py - Creating agent: FeedbackAgent
2024-05-30 16:36:32,732 - cauldron - INFO   - agent_defs.py - Agent FeedbackAgent created with node functools.partial(<function agent_node at 0x000002B0C9801D00>, agent=ChatPromptTemplate(input_variables=['messages'], input_types={'messages': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'system_message': 'You are the Feedback Interpreter node for the Cauldron application. Your task is to interpret feedback from users and other nodes, identifying areas for recipe refinement. Analyze the feedback to suggest actionable changes. Ensure all outputs follow Pydantic standards and format them accordingly. Forward your results to the relevant nodes (e.g., Recipe Modification Manager, Flavor Profiling). Clearly address any looping issues or need for further input.', 'tool_names': 'suggest_mod'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['system_message', 'tool_names'], template="You are a helpful AI assistant, collaborating with other assistants. Use the provided tools to progress towards answering the question. If you are unable to fully answer, that's OK, another assistant with different tools  will help where you left off. Execute what you can to make progress. If you or any of the other assistants have the final answer or deliverable, prefix your response with FINAL ANSWER so the team knows to stop. You have access to the following tools: {tool_names}.\n\n Your responsibility is the following:\n{system_message}")), MessagesPlaceholder(variable_name='messages')])
| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000002B0CBC54B00>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000002B0CBC562D0>, openai_api_key=SecretStr('**********'), openai_proxy=''), kwargs={'tools': [{'type': 'function', 'function': {'name': 'suggest_mod', 'description': "suggest_mod(mod_json: Annotated[str, 'The modification JSON to be suggested.'], mods_list_file: Annotated[str, 'The filename for the mods list.'] = 'mods_list.pkl') -> Annotated[str, 'The updated list of modifications.'] - Suggest a new modification to be added to the mods list.", 'parameters': {'type': 'object', 'properties': {'mod_json': {'type': 'string'}, 'mods_list_file': {'default': 'mods_list.pkl', 'type': 'string'}}, 'required': ['mod_json']}}}]}), name='FeedbackAgent')
2024-05-30 16:36:32,735 - cauldron - INFO   - agent_defs.py - Creating agent: SQLAgent
2024-05-30 16:36:32,737 - cauldron - INFO   - agent_defs.py - Agent SQLAgent created with node functools.partial(<function agent_node at 0x000002B0C9801D00>, agent=ChatPromptTemplate(input_variables=['messages'], input_types={'messages': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'system_message': 'You are the SQL Database Retrieval node for the Cauldron application. Your task is to retrieve stored recipe information from the SQL database based on the queries received from other nodes. Ensure all outputs follow Pydantic standards and format them accordingly. Forward the retrieved data to the requesting nodes (e.g., Recipe Modification Manager, Recipe Development Tracker). Clearly communicate if additional input or clarification is needed.', 'tool_names': 'suggest_mod'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['system_message', 'tool_names'], template="You are a helpful AI assistant, collaborating with other assistants. Use the provided tools to progress towards answering the question. If you are unable to fully answer, that's OK, another assistant with different tools  will help where you left off. Execute what you can to make progress. If you or any of the other assistants have the final answer or deliverable, prefix your response with FINAL ANSWER so the team knows to stop. You have access to the following tools: {tool_names}.\n\n Your responsibility is the following:\n{system_message}")), MessagesPlaceholder(variable_name='messages')])
| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000002B0CBC54B00>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000002B0CBC562D0>, openai_api_key=SecretStr('**********'), openai_proxy=''), kwargs={'tools': [{'type': 'function', 'function': {'name': 'suggest_mod', 'description': "suggest_mod(mod_json: Annotated[str, 'The modification JSON to be suggested.'], mods_list_file: Annotated[str, 'The filename for the mods list.'] = 'mods_list.pkl') -> Annotated[str, 'The updated list of modifications.'] - Suggest a new modification to be added to the mods list.", 'parameters': {'type': 'object', 'properties': {'mod_json': {'type': 'string'}, 'mods_list_file': {'default': 'mods_list.pkl', 'type': 'string'}}, 'required': ['mod_json']}}}]}), name='SQLAgent')
2024-05-30 16:36:32,740 - cauldron - INFO   - agent_defs.py - Creating agent: ModificationsAgent
2024-05-30 16:36:32,745 - cauldron - INFO   - agent_defs.py - Agent ModificationsAgent created with node functools.partial(<function agent_node at 0x000002B0C9801D00>, agent=ChatPromptTemplate(input_variables=['messages'], input_types={'messages': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'system_message': 'You are the Recipe Modification Manager node for the Cauldron application. Your task is to manage modifications to the recipe based on inputs from other nodes. Analyze suggestions from Flavor Profiling, Nutritional Balancing, Cost & Availability, and Feedback Interpreter to update the recipe accordingly. Ensure all outputs follow Pydantic standards and format them accordingly. Forward the updated recipe to the relevant nodes (e.g., Recipe Development Tracker). Address any looping issues or need for further input clearly.', 'tool_names': 'suggest_mod, get_mods_list, push_mod, rank_mod, remove_mod'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['system_message', 'tool_names'], template="You are a helpful AI assistant, collaborating with other assistants. Use the provided tools to progress towards answering the question. If you are unable to fully answer, that's OK, another assistant with different tools  will help where you left off. Execute what you can to make progress. If you or any of the other assistants have the final answer or deliverable, prefix your response with FINAL ANSWER so the team knows to stop. You have access to the following tools: {tool_names}.\n\n Your responsibility is the following:\n{system_message}")), MessagesPlaceholder(variable_name='messages')])
| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000002B0CBC54B00>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000002B0CBC562D0>, openai_api_key=SecretStr('**********'), openai_proxy=''), kwargs={'tools': [{'type': 'function', 'function': {'name': 'suggest_mod', 'description': "suggest_mod(mod_json: Annotated[str, 'The modification JSON to be suggested.'], mods_list_file: Annotated[str, 'The filename for the mods list.'] = 'mods_list.pkl') -> Annotated[str, 'The updated list of modifications.'] - Suggest a new modification to be added to the mods list.", 'parameters': {'type': 'object', 'properties': {'mod_json': {'type': 'string'}, 'mods_list_file': {'default': 'mods_list.pkl', 'type': 'string'}}, 'required': ['mod_json']}}}, {'type': 'function', 'function': {'name': 'get_mods_list', 'description': "get_mods_list(mods_list_file: Annotated[str, 'The filename for the mods list.'] = 'mods_list.pkl') -> Annotated[str, 'The current list of suggested modifications.'] - Get the current list of suggested modifications.", 'parameters': {'type': 'object', 'properties': {'mods_list_file': {'default': 'mods_list.pkl', 'type': 'string'}}}}}, {'type': 'function', 'function': {'name': 'push_mod', 'description': "push_mod(mods_list_file: Annotated[str, 'The filename for the mods list.'] = 'mods_list.pkl') -> Annotated[str, 'The modification that was applied, if available.'] - Apply the top modification from the mods list.", 'parameters': {'type': 'object', 'properties': {'mods_list_file': {'default': 'mods_list.pkl', 'type': 'string'}}}}}, {'type': 'function', 'function': {'name': 'rank_mod', 'description': "rank_mod(mod_id: Annotated[str, 'The ID of the modification to reprioritize.'], new_priority: Annotated[int, 'The new priority for the modification (1 = highest priority, larger numbers = lower priority).'], mods_list_file: Annotated[str, 'The filename for the mods list.'] = 'mods_list.pkl') -> Annotated[str, 'The updated list of modifications.'] - Reprioritize a given modification within the mods list.\n\n    The priority ranking options are as follows:\n    - A lower numerical value indicates higher priority (e.g., 1 is the highest priority).\n    - Larger numerical values indicate lower priority.", 'parameters': {'type': 'object', 'properties': {'mod_id': {'type': 'string'}, 'new_priority': {'type': 'integer'}, 'mods_list_file': {'default': 'mods_list.pkl', 'type': 'string'}}, 'required': ['mod_id', 'new_priority']}}}, {'type': 'function', 'function': {'name': 'remove_mod', 'description': "remove_mod(mods_list_file: Annotated[str, 'The filename for the mods list.'], mod_id: Annotated[str, 'The ID of the modification to remove.']) -> Annotated[str, 'Confirmation message of removal status.'] - Remove a modification from the mods list.", 'parameters': {'type': 'object', 'properties': {'mods_list_file': {'type': 'string'}, 'mod_id': {'type': 'string'}}, 'required': ['mods_list_file', 'mod_id']}}}]}), name='ModificationsAgent')
2024-05-30 16:36:32,752 - cauldron - INFO   - agent_defs.py - Creating agent: DevelopmentTrackerAgent
2024-05-30 16:36:32,755 - cauldron - INFO   - agent_defs.py - Agent DevelopmentTrackerAgent created with node functools.partial(<function agent_node at 0x000002B0C9801D00>, agent=ChatPromptTemplate(input_variables=['messages'], input_types={'messages': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'system_message': 'You are the Recipe Development Tracker node for the Cauldron application. Your task is to plot and track the development process of the recipe, documenting all changes and decisions made by other nodes. Ensure that the development path is clear and logical. Format all documentation following Pydantic standards. Provide detailed logs to the relevant nodes and the Conductor node for transparency. Address any looping issues or need for further input clearly and concisely.', 'tool_names': 'create_recipe_graph, get_recipe, add_node, get_foundational_recipe, get_graph'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['system_message', 'tool_names'], template="You are a helpful AI assistant, collaborating with other assistants. Use the provided tools to progress towards answering the question. If you are unable to fully answer, that's OK, another assistant with different tools  will help where you left off. Execute what you can to make progress. If you or any of the other assistants have the final answer or deliverable, prefix your response with FINAL ANSWER so the team knows to stop. You have access to the following tools: {tool_names}.\n\n Your responsibility is the following:\n{system_message}")), MessagesPlaceholder(variable_name='messages')])
| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000002B0CBC54B00>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000002B0CBC562D0>, openai_api_key=SecretStr('**********'), openai_proxy=''), kwargs={'tools': [{'type': 'function', 'function': {'name': 'create_recipe_graph', 'description': "create_recipe_graph(recipe_json: Annotated[str, 'The JSON representation of the foundational recipe.'], graph_file: Annotated[str, 'The filename for the recipe graph.']) -> Annotated[str, 'ID of the newly created foundational recipe node.'] - Create a new recipe graph with the provided foundational recipe.", 'parameters': {'type': 'object', 'properties': {'recipe_json': {'type': 'string'}, 'graph_file': {'type': 'string'}}, 'required': ['recipe_json', 'graph_file']}}}, {'type': 'function', 'function': {'name': 'get_recipe', 'description': "get_recipe(graph_file: Annotated[str, 'The filename for the recipe graph.'], node_id: Annotated[Optional[str], 'The node ID of the recipe to retrieve. If not provided, retrieves the foundational recipe.'] = None) -> Annotated[str, 'The JSON representation of the recipe.'] - Get the JSON representation of the recipe at the specified node ID.", 'parameters': {'type': 'object', 'properties': {'graph_file': {'type': 'string'}, 'node_id': {'type': 'string'}}, 'required': ['graph_file']}}}, {'type': 'function', 'function': {'name': 'add_node', 'description': "add_node(recipe_json: Annotated[str, 'The JSON representation of the recipe to add.'], graph_file: Annotated[str, 'The filename for the recipe graph.']) -> Annotated[str, 'ID of the newly added recipe node.'] - Add a new node to the recipe graph with the provided recipe and create an edge from the current foundational recipe.", 'parameters': {'type': 'object', 'properties': {'recipe_json': {'type': 'string'}, 'graph_file': {'type': 'string'}}, 'required': ['recipe_json', 'graph_file']}}}, {'type': 'function', 'function': {'name': 'get_foundational_recipe', 'description': "get_foundational_recipe(graph_file: Annotated[str, 'The filename for the recipe graph.']) -> Annotated[str, 'The JSON representation of the current foundational recipe.'] - Get the JSON representation of the current foundational recipe.", 'parameters': {'type': 'object', 'properties': {'graph_file': {'type': 'string'}}, 'required': ['graph_file']}}}, {'type': 'function', 'function': {'name': 'get_graph', 'description': "get_graph(graph_file: Annotated[str, 'The filename for the recipe graph.']) -> Annotated[str, 'A representation of the current recipe graph.'] - Get a representation of the current recipe graph.", 'parameters': {'type': 'object', 'properties': {'graph_file': {'type': 'string'}}, 'required': ['graph_file']}}}]}), name='DevelopmentTrackerAgent')
2024-05-30 16:36:32,761 - cauldron - INFO   - agent_defs.py - Creating agent: PeripheralFeedbackAgent
2024-05-30 16:36:32,763 - cauldron - INFO   - agent_defs.py - Agent PeripheralFeedbackAgent created with node functools.partial(<function agent_node at 0x000002B0C9801D00>, agent=ChatPromptTemplate(input_variables=['messages'], input_types={'messages': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'system_message': 'You are the Peripheral Interpreter node for the Cauldron application. Your task is to interpret feedback from connected smart devices (e.g., smart ovens, kitchen scales) and provide actionable insights to other nodes. Ensure all outputs follow Pydantic standards and format them accordingly. Forward the insights to the relevant nodes (e.g., Feedback Interpreter, Recipe Modification Manager). Clearly address any looping issues or need for further input.', 'tool_names': 'suggest_mod'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['system_message', 'tool_names'], template="You are a helpful AI assistant, collaborating with other assistants. Use the provided tools to progress towards answering the question. If you are unable to fully answer, that's OK, another assistant with different tools  will help where you left off. Execute what you can to make progress. If you or any of the other assistants have the final answer or deliverable, prefix your response with FINAL ANSWER so the team knows to stop. You have access to the following tools: {tool_names}.\n\n Your responsibility is the following:\n{system_message}")), MessagesPlaceholder(variable_name='messages')])
| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000002B0CBC54B00>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000002B0CBC562D0>, openai_api_key=SecretStr('**********'), openai_proxy=''), kwargs={'tools': [{'type': 'function', 'function': {'name': 'suggest_mod', 'description': "suggest_mod(mod_json: Annotated[str, 'The modification JSON to be suggested.'], mods_list_file: Annotated[str, 'The filename for the mods list.'] = 'mods_list.pkl') -> Annotated[str, 'The updated list of modifications.'] - Suggest a new modification to be added to the mods list.", 'parameters': {'type': 'object', 'properties': {'mod_json': {'type': 'string'}, 'mods_list_file': {'default': 'mods_list.pkl', 'type': 'string'}}, 'required': ['mod_json']}}}]}), name='PeripheralFeedbackAgent')
2024-05-30 16:36:32,767 - cauldron - INFO   - agent_defs.py - All agents created.
